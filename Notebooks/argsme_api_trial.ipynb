{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/balazs/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/balazs/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from gensim.models import KeyedVectors\n",
    "import scipy\n",
    "from collections import Counter\n",
    "from textblob import TextBlob\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search API function\n",
    "def search_api(root, output_dir, pageSize, url_template):\n",
    "    # Loop through topics and search for the top 1000 arguments\n",
    "    for topic in root.findall('topic'):\n",
    "        topic_number = topic.find('number')\n",
    "        if topic_number is not None:\n",
    "            topic_id = topic_number.text\n",
    "            topic_title = topic.find('title')\n",
    "            if topic_title is not None and topic_title.text:\n",
    "                topic_title = topic_title.text.strip()\n",
    "            else:\n",
    "                topic_title = ''\n",
    "                \n",
    "            url = url_template.format(topic_title, pageSize)\n",
    "\n",
    "            # Execute the search query using API\n",
    "            response = requests.get(url).json()\n",
    "            results = response['arguments']\n",
    "            print(len(results))\n",
    "            \n",
    "            # Save the top 1000 arguments for the current topic to a file\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "            topic_file_name = os.path.join(output_dir, f\"{topic_id}.csv\")\n",
    "            header = ['id', 'conclusion', 'premises', 'stance', 'rank']\n",
    "            arguments = []\n",
    "            for i, result in enumerate(results):\n",
    "                argument = [result['id'], result['conclusion'], result['premises'][0]['text'], result['stance'], i + 1]\n",
    "                arguments.append(argument)\n",
    "            \n",
    "            retrieved_arguments_pd = pd.DataFrame(arguments, columns=header)\n",
    "            retrieved_arguments_pd.to_csv(topic_file_name, sep='\\t', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create runfiles using API\n",
    "def make_runfile(root, output_dir, pageSize, url_template, run_tag):\n",
    "    # Create the output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    # Set the output file name\n",
    "    output_file_name = os.path.join(output_dir, \"runfile21.txt\")\n",
    "    # Open the output file for writing\n",
    "    with open(output_file_name, 'w') as f:\n",
    "        # Loop through topics and search for the top 50 arguments\n",
    "        for topic in root.findall('topic'):\n",
    "            topic_number = topic.find('number')\n",
    "            if topic_number is not None:\n",
    "                topic_id = topic_number.text\n",
    "                topic_title = topic.find('title')\n",
    "                if topic_title is not None and topic_title.text:\n",
    "                    topic_title = topic_title.text.strip()\n",
    "                else:\n",
    "                    topic_title = ''\n",
    "                    \n",
    "                # Format the search URL with the topic title and page size\n",
    "                url = url_template.format(topic_title, pageSize)\n",
    "\n",
    "                # Execute the search query\n",
    "                response = requests.get(url).json()\n",
    "                # Get the top 50 results\n",
    "                results = response['arguments'][:50]\n",
    "                print (len(results))\n",
    "                \n",
    "                # Loop through the top 50 results\n",
    "                for i, result in enumerate(results):\n",
    "                    # Get the argument ID, rank, and score\n",
    "                    argument_id = result['id']\n",
    "                    rank = i + 1\n",
    "                    score = result['explanation']['score']\n",
    "                    # Format the output line\n",
    "                    line = f\"{topic_id} Q0 {argument_id} {rank} {score} {run_tag}\\n\"\n",
    "                    # Write the output line to the file\n",
    "                    f.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\ndef extract_features(directory):\\n    corpus = []\\n    for filename in os.listdir(directory):\\n        if filename.endswith(\\'.txt\\'):\\n            with open(os.path.join(directory, filename), \\'r\\') as f:\\n                corpus.append(f.read())\\n\\n    print(f\\'Number of documents in corpus: {len(corpus)}\\')\\n\\n    sia = SentimentIntensityAnalyzer()\\n    sentiment_features = [sia.polarity_scores(doc) for doc in corpus]\\n    \\n    sentiment_keys = [\\'neg\\', \\'neu\\', \\'pos\\', \\'compound\\']\\n    X_sentiment = np.array([[doc[key] for key in sentiment_keys] for doc in sentiment_features])\\n\\n    vectorizer = TfidfVectorizer(min_df=5, max_df=0.8)\\n    tfidf_features = vectorizer.fit_transform(corpus).toarray()\\n\\n    tokenized_corpus = [word_tokenize(doc) for doc in corpus]\\n    \\n    model = Word2Vec(tokenized_corpus, vector_size=250, min_count=1)\\n    \\n    print(f\\'Vocabulary size: {len(model.wv)}\\')\\n    print(f\\'Embedding dimension: {model.vector_size}\\')\\n\\n    w2v_features = []\\n    for doc in tokenized_corpus:\\n        doc_vectors = [model.wv[word] for word in doc if word in model.wv]\\n        if len(doc_vectors) > 0:\\n            doc_features = np.mean(doc_vectors, axis=0)\\n        else:\\n            doc_features = np.zeros(model.vector_size)\\n        w2v_features.append(doc_features)\\n\\n    X_tfidf = np.array(tfidf_features)\\n    X_w2v = np.array(w2v_features)\\n\\n    X_combined = np.concatenate((X_sentiment, X_tfidf, X_w2v), axis=1)\\n\\n    df = pd.DataFrame(X_combined)\\n    df.to_csv(\\'extracted_features.csv\\', index=False)\\n\\n    print(\"successful extraction\")\\n    return X_combined\\n'"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' \n",
    "def extract_features(directory):\n",
    "    corpus = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith('.txt'):\n",
    "            with open(os.path.join(directory, filename), 'r') as f:\n",
    "                corpus.append(f.read())\n",
    "\n",
    "    print(f'Number of documents in corpus: {len(corpus)}')\n",
    "\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    sentiment_features = [sia.polarity_scores(doc) for doc in corpus]\n",
    "    \n",
    "    sentiment_keys = ['neg', 'neu', 'pos', 'compound']\n",
    "    X_sentiment = np.array([[doc[key] for key in sentiment_keys] for doc in sentiment_features])\n",
    "\n",
    "    vectorizer = TfidfVectorizer(min_df=5, max_df=0.8)\n",
    "    tfidf_features = vectorizer.fit_transform(corpus).toarray()\n",
    "\n",
    "    tokenized_corpus = [word_tokenize(doc) for doc in corpus]\n",
    "    \n",
    "    model = Word2Vec(tokenized_corpus, vector_size=250, min_count=1)\n",
    "    \n",
    "    print(f'Vocabulary size: {len(model.wv)}')\n",
    "    print(f'Embedding dimension: {model.vector_size}')\n",
    "\n",
    "    w2v_features = []\n",
    "    for doc in tokenized_corpus:\n",
    "        doc_vectors = [model.wv[word] for word in doc if word in model.wv]\n",
    "        if len(doc_vectors) > 0:\n",
    "            doc_features = np.mean(doc_vectors, axis=0)\n",
    "        else:\n",
    "            doc_features = np.zeros(model.vector_size)\n",
    "        w2v_features.append(doc_features)\n",
    "\n",
    "    X_tfidf = np.array(tfidf_features)\n",
    "    X_w2v = np.array(w2v_features)\n",
    "\n",
    "    X_combined = np.concatenate((X_sentiment, X_tfidf, X_w2v), axis=1)\n",
    "\n",
    "    df = pd.DataFrame(X_combined)\n",
    "    df.to_csv('extracted_features.csv', index=False)\n",
    "\n",
    "    print(\"successful extraction\")\n",
    "    return X_combined\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def extract_features(directory):\n",
    "    corpus = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith('.txt'):\n",
    "            with open(os.path.join(directory, filename), 'r') as f:\n",
    "                corpus.append(f.read())\n",
    "\n",
    "    print(f'Number of documents in corpus: {len(corpus)}')\n",
    "\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    sentiment_features = [sia.polarity_scores(doc) for doc in corpus]\n",
    "    \n",
    "    sentiment_keys = ['neg', 'neu', 'pos', 'compound']\n",
    "    X_sentiment = np.array([[doc[key] for key in sentiment_keys] for doc in sentiment_features])\n",
    "\n",
    "    vectorizer = TfidfVectorizer(min_df=5, max_df=0.8)\n",
    "    tfidf_features = vectorizer.fit_transform(corpus).toarray()\n",
    "\n",
    "    tokenized_corpus = [word_tokenize(doc) for doc in corpus]\n",
    "    \n",
    "    model = Word2Vec(tokenized_corpus, vector_size=100, min_count=1)\n",
    "    \n",
    "    print(f'Vocabulary size: {len(model.wv)}')\n",
    "    print(f'Embedding dimension: {model.vector_size}')\n",
    "\n",
    "    w2v_features = []\n",
    "    for doc in tokenized_corpus:\n",
    "        doc_vectors = [model.wv[word] for word in doc if word in model.wv]\n",
    "        if len(doc_vectors) > 0:\n",
    "            doc_features = np.mean(doc_vectors, axis=0)\n",
    "        else:\n",
    "            doc_features = np.zeros(model.vector_size)\n",
    "        w2v_features.append(doc_features)\n",
    "\n",
    "    X_tfidf = np.array(tfidf_features)\n",
    "    X_w2v = np.array(w2v_features)\n",
    "\n",
    "    # Additional features using TextBlob\n",
    "    polarity = [TextBlob(doc).sentiment.polarity for doc in corpus]\n",
    "    subjectivity = [TextBlob(doc).sentiment.subjectivity for doc in corpus]\n",
    "    \n",
    "    X_combined = np.concatenate((X_sentiment, X_tfidf, X_w2v, np.array(polarity).reshape(-1,1), np.array(subjectivity).reshape(-1,1)), axis=1)\n",
    "\n",
    "    df = pd.DataFrame(X_combined)\n",
    "    df.to_csv('extracted_features.csv', index=False)\n",
    "\n",
    "    print(\"successful extraction\")\n",
    "    return X_combined\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(directory):\n",
    "    corpus = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith('.txt'):\n",
    "            with open(os.path.join(directory, filename), 'r') as f:\n",
    "                corpus.append(f.read())\n",
    "\n",
    "    print(f'Number of documents in corpus: {len(corpus)}')\n",
    "\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    sentiment_features = [sia.polarity_scores(doc) for doc in corpus]\n",
    "    \n",
    "    sentiment_keys = ['neg', 'neu', 'pos', 'compound']\n",
    "    X_sentiment = np.array([[doc[key] for key in sentiment_keys] for doc in sentiment_features])\n",
    "\n",
    "    vectorizer = TfidfVectorizer(min_df=5, max_df=0.8)\n",
    "    tfidf_features = vectorizer.fit_transform(corpus).toarray()\n",
    "\n",
    "    tokenized_corpus = [word_tokenize(doc) for doc in corpus]\n",
    "    \n",
    "    model = Word2Vec(tokenized_corpus, vector_size=200, min_count=1)\n",
    "    \n",
    "    print(f'Vocabulary size: {len(model.wv)}')\n",
    "    print(f'Embedding dimension: {model.vector_size}')\n",
    "\n",
    "    w2v_features = []\n",
    "    for doc in tokenized_corpus:\n",
    "        doc_vectors = [model.wv[word] for word in doc if word in model.wv]\n",
    "        if len(doc_vectors) > 0:\n",
    "            doc_features = np.mean(doc_vectors, axis=0)\n",
    "        else:\n",
    "            doc_features = np.zeros(model.vector_size)\n",
    "        w2v_features.append(doc_features)\n",
    "\n",
    "    X_tfidf = np.array(tfidf_features)\n",
    "    X_w2v = np.array(w2v_features)\n",
    "\n",
    "    # Additional features using TextBlob\n",
    "    polarity = [TextBlob(doc).sentiment.polarity for doc in corpus]\n",
    "    subjectivity = [TextBlob(doc).sentiment.subjectivity for doc in corpus]\n",
    "    \n",
    "    # Label the columns of the output CSV file\n",
    "    columns = ['polarity', 'subjectivity'] + sentiment_keys + [f'TF-IDF_{i}' for i in range(X_tfidf.shape[1])] + [f'W2V_{i}' for i in range(X_w2v.shape[1])]\n",
    "\n",
    "    X_combined = np.concatenate((np.array(polarity).reshape(-1,1), np.array(subjectivity).reshape(-1,1), X_sentiment, X_tfidf, X_w2v), axis=1)\n",
    "\n",
    "    df = pd.DataFrame(X_combined, columns=columns)\n",
    "    df.to_csv('extracted_features.csv', index=False)\n",
    "\n",
    "    print(\"successful extraction\")\n",
    "\n",
    "    # Display the distribution of polarity scores\n",
    "    polarity_df = pd.DataFrame(polarity, columns=['polarity'])\n",
    "    polarity_df.plot(kind='hist', alpha=0.5, title='Distribution of polarity scores')\n",
    "    plt.show()\n",
    "\n",
    "    # Display the distribution of subjectivity scores\n",
    "    subjectivity_df = pd.DataFrame(subjectivity, columns=['subjectivity'])\n",
    "    subjectivity_df.plot(kind='hist', alpha=0.5, title='Distribution of subjectivity scores')\n",
    "    plt.show()\n",
    "\n",
    "    # Display the distribution of sentiment scores\n",
    "    sentiment_df = pd.DataFrame(X_sentiment, columns=sentiment_keys)\n",
    "    sentiment_df.plot(kind='hist', alpha=0.5, title='Distribution of sentiment scores')\n",
    "    plt.show()\n",
    "\n",
    "    # Display the most common words in the corpus\n",
    "    word_freq = Counter([word for doc in tokenized_corpus for word in doc])\n",
    "    most_common_words = word_freq.most_common(10)\n",
    "    print(\"The most common words in the corpus are:\")\n",
    "    for word, count in most_common_words:\n",
    "        print(f\"{word}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the XML files for 2020\n",
    "tree20 = ET.parse('/Users/balazs/Desktop/dissertationProjectCode/dissertationCodeBase/Data/topic_files/topics-task-1-2020.xml')\n",
    "root20 = tree20.getroot()\n",
    "\n",
    "# Load the XML files for 2021\n",
    "tree21 = ET.parse('/Users/balazs/Desktop/dissertationProjectCode/dissertationCodeBase/Data/topic_files/topics-task-1-only-titles.xml')\n",
    "root21 = tree21.getroot()\n",
    "\n",
    "# Define the output directories for 2020\n",
    "output_dir20 = \"/Users/balazs/Desktop/dissertationProjectCode/dissertationCodeBase/Data/args20api/\"\n",
    "\n",
    "# Define the output directories for 2021\n",
    "output_dir21 = \"/Users/balazs/Desktop/dissertationProjectCode/dissertationCodeBase/Data/args21api/\"\n",
    "\n",
    "pageSize = 1000\n",
    "url_template = \"https://args.me/api/v2/arguments?query={}&corpora=args-me-2020-04-01&pageSize={}&format=json\"\n",
    "#url_template = \"https://args.me/api/v2/arguments?query={}&pageSize={}&format=json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41\n",
      "6\n",
      "0\n",
      "0\n",
      "0\n",
      "1000\n",
      "51\n",
      "84\n",
      "27\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "7\n",
      "0\n",
      "0\n",
      "6\n",
      "0\n",
      "8\n",
      "0\n",
      "214\n",
      "29\n",
      "15\n",
      "0\n",
      "14\n",
      "0\n",
      "0\n",
      "0\n",
      "42\n",
      "69\n",
      "0\n",
      "203\n",
      "0\n",
      "0\n",
      "151\n",
      "2\n",
      "339\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "22\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "6\n",
      "148\n",
      "0\n",
      "0\n",
      "0\n",
      "162\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "802\n",
      "0\n",
      "10\n",
      "0\n",
      "23\n",
      "1000\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "8\n",
      "0\n",
      "54\n",
      "126\n",
      "1000\n",
      "10\n",
      "14\n",
      "326\n",
      "0\n",
      "0\n",
      "160\n",
      "0\n",
      "21\n",
      "0\n",
      "5\n",
      "2\n",
      "8\n",
      "0\n",
      "2\n",
      "0\n",
      "8\n",
      "0\n",
      "2\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# Call search functions for 2020\n",
    "search_api(root20, output_dir20, pageSize, url_template)\n",
    "\n",
    "#Call search functions for 2021\n",
    "search_api(root21, output_dir21, pageSize, url_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41\n",
      "6\n",
      "0\n",
      "0\n",
      "0\n",
      "50\n",
      "50\n",
      "50\n",
      "27\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "7\n",
      "0\n",
      "0\n",
      "6\n",
      "0\n",
      "8\n",
      "0\n",
      "50\n",
      "29\n",
      "15\n",
      "0\n",
      "14\n",
      "0\n",
      "0\n",
      "0\n",
      "42\n",
      "50\n",
      "0\n",
      "50\n",
      "0\n",
      "0\n",
      "50\n",
      "2\n",
      "50\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "22\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "6\n",
      "50\n",
      "0\n",
      "0\n",
      "0\n",
      "50\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "50\n",
      "0\n",
      "10\n",
      "0\n",
      "23\n",
      "50\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "8\n",
      "0\n",
      "50\n",
      "50\n",
      "50\n",
      "10\n",
      "14\n",
      "50\n",
      "0\n",
      "0\n",
      "50\n",
      "0\n",
      "21\n",
      "0\n",
      "5\n",
      "2\n",
      "8\n",
      "0\n",
      "2\n",
      "0\n",
      "8\n",
      "0\n",
      "2\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# Call search functions for 2020\n",
    "make_runfile(root20, output_dir20, pageSize, url_template, \"40323335_Run_File_20\")\n",
    "\n",
    "# Call search functions for 2021\n",
    "make_runfile(root21, output_dir21, pageSize, url_template, \"40323335_Run_File_21\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents in corpus: 50\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[217], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m directory \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m/Users/balazs/Desktop/dissertationProjectCode/dissertationCodeBase/Data/args20api\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m----> 2\u001b[0m features20 \u001b[39m=\u001b[39m extract_features(directory)\n",
      "Cell \u001b[0;32mIn[216], line 11\u001b[0m, in \u001b[0;36mextract_features\u001b[0;34m(directory)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mNumber of documents in corpus: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(corpus)\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m     10\u001b[0m sia \u001b[39m=\u001b[39m SentimentIntensityAnalyzer()\n\u001b[0;32m---> 11\u001b[0m sentiment_features \u001b[39m=\u001b[39m [sia\u001b[39m.\u001b[39mpolarity_scores(doc) \u001b[39mfor\u001b[39;00m doc \u001b[39min\u001b[39;00m corpus]\n\u001b[1;32m     13\u001b[0m sentiment_keys \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mneg\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mneu\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mpos\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mcompound\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     14\u001b[0m X_sentiment \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([[doc[key] \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m sentiment_keys] \u001b[39mfor\u001b[39;00m doc \u001b[39min\u001b[39;00m sentiment_features])\n",
      "Cell \u001b[0;32mIn[216], line 11\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mNumber of documents in corpus: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(corpus)\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m     10\u001b[0m sia \u001b[39m=\u001b[39m SentimentIntensityAnalyzer()\n\u001b[0;32m---> 11\u001b[0m sentiment_features \u001b[39m=\u001b[39m [sia\u001b[39m.\u001b[39;49mpolarity_scores(doc) \u001b[39mfor\u001b[39;00m doc \u001b[39min\u001b[39;00m corpus]\n\u001b[1;32m     13\u001b[0m sentiment_keys \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mneg\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mneu\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mpos\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mcompound\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     14\u001b[0m X_sentiment \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([[doc[key] \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m sentiment_keys] \u001b[39mfor\u001b[39;00m doc \u001b[39min\u001b[39;00m sentiment_features])\n",
      "File \u001b[0;32m/opt/anaconda3/envs/env_name/lib/python3.9/site-packages/nltk/sentiment/vader.py:368\u001b[0m, in \u001b[0;36mSentimentIntensityAnalyzer.polarity_scores\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    366\u001b[0m \u001b[39mfor\u001b[39;00m item \u001b[39min\u001b[39;00m words_and_emoticons:\n\u001b[1;32m    367\u001b[0m     valence \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m--> 368\u001b[0m     i \u001b[39m=\u001b[39m words_and_emoticons\u001b[39m.\u001b[39;49mindex(item)\n\u001b[1;32m    369\u001b[0m     \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    370\u001b[0m         i \u001b[39m<\u001b[39m \u001b[39mlen\u001b[39m(words_and_emoticons) \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    371\u001b[0m         \u001b[39mand\u001b[39;00m item\u001b[39m.\u001b[39mlower() \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mkind\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    372\u001b[0m         \u001b[39mand\u001b[39;00m words_and_emoticons[i \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mlower() \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mof\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    373\u001b[0m     ) \u001b[39mor\u001b[39;00m item\u001b[39m.\u001b[39mlower() \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconstants\u001b[39m.\u001b[39mBOOSTER_DICT:\n\u001b[1;32m    374\u001b[0m         sentiments\u001b[39m.\u001b[39mappend(valence)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "directory = '/Users/balazs/Desktop/dissertationProjectCode/dissertationCodeBase/Data/args20api'\n",
    "features20 = extract_features(directory)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_name",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "37cc1b05901c8571bea9fc4b42e3f528ca4394d6e6719f1b5d30d208c4a3cfc1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
