{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Done for Paper\n",
    "import unicodedata\n",
    "\n",
    "\n",
    "def whitespace_tokenize(text,split):\n",
    "\t\"\"\"Runs basic whitespace cleaning and splitting on a piece of text.\"\"\"\n",
    "\ttext = text.strip()\n",
    "\tif not text:\n",
    "\t\tif split:\n",
    "\t\t\treturn []\n",
    "\t\telse:\n",
    "\t\t\treturn \"\"\n",
    "\tif split:\n",
    "\t\treturn text.split()\n",
    "\treturn text\n",
    "\n",
    "def _is_whitespace(char):\n",
    "\t\"\"\"Checks whether `chars` is a whitespace character.\"\"\"\n",
    "\t# \\t, \\n, and \\r are technically contorl characters but we treat them\n",
    "\t# # as whitespace since they are generally considered as such.\n",
    "\tif char == \" \" or char == \"\\t\" or char == \"\\n\" or char == \"\\r\":\n",
    "\t\treturn True\n",
    "\tcat = unicodedata.category(char)\n",
    "\tif cat == \"Zs\":\n",
    "\t\treturn True\n",
    "\treturn False\n",
    "\n",
    "\n",
    "def _is_control(char):\n",
    "\t\"\"\"Checks whether `chars` is a control character.\"\"\"\n",
    "\t# These are technically control characters but we count them as whitespace characters.\n",
    "\n",
    "\tif char == \"\\t\" or char == \"\\n\" or char == \"\\r\":\n",
    "\t\treturn False\n",
    "\tcat = unicodedata.category(char)\n",
    "\tif cat in (\"Cc\", \"Cf\"):\n",
    "\t\treturn True\n",
    "\treturn False\n",
    "\n",
    "\n",
    "def _is_punctuation(char):\n",
    "\t\"\"\"Checks whether `chars` is a punctuation character.\"\"\"\n",
    "\tcp = ord(char)\n",
    "\t# We treat all non-letter/number ASCII as punctuation.\n",
    "\t# Characters such as \"^\", \"$\", and \"`\" are not in the Unicode\n",
    "\t# Punctuation class but we treat them as punctuation anyways, for\n",
    "\t# consistency.\n",
    "\tif ((cp >= 33 and cp <= 47) or (cp >= 58 and cp <= 64) or\n",
    "\t\t(cp >= 91 and cp <= 96) or (cp >= 123 and cp <= 126)):\n",
    "\t\treturn True\n",
    "\tcat = unicodedata.category(char)\n",
    "\tif cat.startswith(\"P\"):\n",
    "\t\treturn True\n",
    "\treturn False\n",
    "\n",
    "class BasicTokenizer(object):\n",
    "\t\"\"\"Runs basic tokenization (punctuation splitting, lower casing, etc.).\"\"\"\n",
    "\n",
    "\tdef __init__(self, do_lower_case=True):\n",
    "\t\t\"\"\"Constructs a BasicTokenizer.\n",
    "\t\tArgs: do_lower_case: Whether to lower case the input.\n",
    "\t\t\"\"\"\n",
    "\t\tself.do_lower_case = do_lower_case\n",
    "\n",
    "\tdef tokenize(self, text):\n",
    "\t\t\"\"\"Tokenizes a piece of text.\"\"\"\n",
    "\t\t# text = convert_to_unicode(text)\n",
    "\t\ttext = self._clean_text(text)\n",
    "\t\torig_tokens = whitespace_tokenize(text, split=True)\n",
    "\t\tsplit_tokens = []\n",
    "\t\tfor token in orig_tokens:\n",
    "\t\t\tif self.do_lower_case:\n",
    "\t\t\t\ttoken = token.lower()\n",
    "\t\t\t\ttoken = self._run_strip_accents(token)\n",
    "\t\t\tsplit_tokens.extend(self._run_split_on_punc(token))\n",
    "\t\toutput_text = whitespace_tokenize(\" \".join(split_tokens), split=False)\n",
    "\t\treturn output_text\n",
    "\n",
    "\tdef _run_strip_accents(self, text):\n",
    "\t\t\"\"\"Strips accents from a piece of text.\"\"\"\n",
    "\t\ttext = unicodedata.normalize(\"NFD\", text)\n",
    "\t\toutput = []\n",
    "\t\tfor char in text:\n",
    "\t\t\tcat = unicodedata.category(char)\n",
    "\t\t\tif cat == \"Mn\":\n",
    "\t\t\t\tcontinue\n",
    "\t\t\toutput.append(char)\n",
    "\t\treturn \"\".join(output)\n",
    "\t\n",
    "\tdef _run_split_on_punc(self, text):\n",
    "\t\t\"\"\"Splits punctuation on a piece of text.\"\"\"\n",
    "\t\tchars = list(text)\n",
    "\t\ti = 0\n",
    "\t\tstart_new_word = True\n",
    "\t\toutput = []\n",
    "\t\twhile i < len(chars):\n",
    "\t\t\tchar = chars[i]\n",
    "\t\t\tif _is_punctuation(char):\n",
    "\t\t\t\toutput.append([char])\n",
    "\t\t\t\tstart_new_word = True\n",
    "\t\t\telse:\n",
    "\t\t\t\tif start_new_word:\n",
    "\t\t\t\t\toutput.append([])\n",
    "\t\t\t\tstart_new_word = False\n",
    "\t\t\t\toutput[-1].append(char)\n",
    "\t\t\ti += 1\n",
    "\n",
    "\t\treturn [\"\".join(x) for x in output]\n",
    "\n",
    "\tdef _clean_text(self, text):\n",
    "\t\t\"\"\"Performs invalid character removal and whitespace cleanup on text.\"\"\"\n",
    "\t\toutput = []\n",
    "\t\tfor char in text:\n",
    "\t\t\tcp = ord(char)\n",
    "\t\t\tif cp == 0 or cp == 0xfffd or _is_control(char):\n",
    "\t\t\t\tcontinue\n",
    "\t\t\tif _is_whitespace(char):\n",
    "\t\t\t\toutput.append(\" \")\n",
    "\t\t\telse:\n",
    "\t\t\t\toutput.append(char)\n",
    "\t\treturn \"\".join(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Load the JSON file\n",
    "try:\n",
    "    with open('/Users/balazs/Desktop/debateorg.json', 'r') as f:\n",
    "        data = json.load(f)\n",
    "except Exception as e:\n",
    "    print(\"Error reading JSON file:\", str(e))\n",
    "    # Handle the error appropriately\n",
    "\n",
    "# Extract the relevant field that you want to tokenize\n",
    "if isinstance(data, list):\n",
    "    text_list = [d['text'] for d in data]\n",
    "else:\n",
    "    print(\"Error: JSON file does not contain a list of dictionaries.\")\n",
    "    # Handle the error appropriately\n",
    "\n",
    "# Create an instance of the BasicTokenizer class\n",
    "tokenizer = BasicTokenizer()\n",
    "\n",
    "# Tokenize the text and write to a new file in the JSONL format\n",
    "try:\n",
    "    with open('/Users/balazs/Desktop/debateorg.json', 'w') as f:\n",
    "        for text in text_list:\n",
    "            tokens = tokenizer.tokenize(text)\n",
    "            output_dict = {'text': ' '.join(tokens)}\n",
    "            json.dump(output_dict, f)\n",
    "            f.write('\\n')\n",
    "except Exception as e:\n",
    "    print(\"Error writing JSONL file:\", str(e))\n",
    "    # Handle the error appropriately"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_name",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "37cc1b05901c8571bea9fc4b42e3f528ca4394d6e6719f1b5d30d208c4a3cfc1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
